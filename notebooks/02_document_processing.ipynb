{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e32bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PALMPAY\\anaconda3\\envs\\findocgpt-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Document Processing Pipeline Started\n",
      "==================================================\n",
      "üìö Loaded 3 sample financial documents\n",
      "  - Apple Inc. 10-K Filing 2023 (802 characters)\n",
      "  - Microsoft Corporation Earnings Report Q4 2023 (661 characters)\n",
      "  - Tesla Inc. Annual Report 2023 (788 characters)\n",
      "Processing: Apple Inc. 10-K Filing 2023\n",
      "  Created 2 chunks\n",
      "Processing: Microsoft Corporation Earnings Report Q4 2023\n",
      "  Created 2 chunks\n",
      "Processing: Tesla Inc. Annual Report 2023\n",
      "  Created 2 chunks\n",
      "\n",
      "‚úÖ Total chunks created: 6\n",
      "üìä Average chunk length: 363 characters\n",
      "\n",
      "üìÑ Sample Chunks:\n",
      "========================================\n",
      "\n",
      "Chunk 1 (doc_0_chunk_0):\n",
      "Company: AAPL\n",
      "Length: 469 characters\n",
      "Content: Apple Inc. Annual Report (Form 10-K) BUSINESS OVERVIEW Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, personal computers, tablets, wearables and accessories, and ...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2 (doc_0_chunk_1):\n",
      "Company: AAPL\n",
      "Length: 314 characters\n",
      "Content: % compared to fiscal 2022. iPhone revenue was $200.6 billion, Services revenue was $85.2 billion, and Mac revenue was $29.4 billion. RISK FACTORS The Company's business is subject to risks including g...\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3 (doc_1_chunk_0):\n",
      "Company: MSFT\n",
      "Length: 467 characters\n",
      "Content: Microsoft Corporation Quarterly Earnings Report FINANCIAL PERFORMANCE Revenue was $56.2 billion and increased 8% year-over-year. Operating income was $24.3 billion and increased 15% year-over-year. Ne...\n",
      "----------------------------------------\n",
      "\n",
      "üß† Initializing embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded all-MiniLM-L6-v2 embedding model\n",
      "\n",
      "üîÑ Generating embeddings for 6 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated embeddings with shape: (6, 384)\n",
      "\n",
      "üìä Embedding Analysis:\n",
      "Embedding dimension: 384\n",
      "Number of document chunks: 6\n",
      "\n",
      "üîó Similarity Analysis:\n",
      "Average similarity: 0.506\n",
      "Max similarity (excluding self): 0.678\n",
      "Min similarity: 0.259\n",
      "\n",
      "‚úÖ No highly similar chunks found - good diversity\n",
      "\n",
      "‚úÖ Vector store created successfully\n",
      "\n",
      "üîç Testing Search Functionality:\n",
      "==================================================\n",
      "\n",
      "üîç Searching for: 'What is the revenue for Apple?'\n",
      "\n",
      "üìã Top 2 Results:\n",
      "\n",
      "1. [AAPL] Apple Inc. 10-K Filing 2023\n",
      "   Similarity: 0.705\n",
      "   Content: % compared to fiscal 2022. iPhone revenue was $200.6 billion, Services revenue was $85.2 billion, and Mac revenue was $29.4 billion. RISK FACTORS The ...\n",
      "\n",
      "2. [AAPL] Apple Inc. 10-K Filing 2023\n",
      "   Similarity: 0.702\n",
      "   Content: Apple Inc. Annual Report (Form 10-K) BUSINESS OVERVIEW Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, personal c...\n",
      "============================================================\n",
      "\n",
      "üîç Searching for: 'Microsoft cloud services performance'\n",
      "\n",
      "üìã Top 2 Results:\n",
      "\n",
      "1. [MSFT] Microsoft Corporation Earnings Report Q4 2023\n",
      "   Similarity: 0.440\n",
      "   Content: ure and other cloud services revenue increased 26%. OUTLOOK The company expects continued growth driven by cloud services adoption and AI integration ...\n",
      "\n",
      "2. [MSFT] Microsoft Corporation Earnings Report Q4 2023\n",
      "   Similarity: 0.408\n",
      "   Content: Microsoft Corporation Quarterly Earnings Report FINANCIAL PERFORMANCE Revenue was $56.2 billion and increased 8% year-over-year. Operating income was ...\n",
      "============================================================\n",
      "\n",
      "üîç Searching for: 'Tesla vehicle deliveries and production'\n",
      "\n",
      "üìã Top 2 Results:\n",
      "\n",
      "1. [TSLA] Tesla Inc. Annual Report 2023\n",
      "   Similarity: 0.691\n",
      "   Content: Tesla Inc. Annual Report BUSINESS OVERVIEW Tesla designs, develops, manufactures, sells and leases high-performance fully electric vehicles and energy...\n",
      "\n",
      "2. [TSLA] Tesla Inc. Annual Report 2023\n",
      "   Similarity: 0.447\n",
      "   Content: ared to 2022. Automotive revenue was $82.4 billion. Energy generation and storage revenue was $6.0 billion. FUTURE OUTLOOK The company aims to achieve...\n",
      "============================================================\n",
      "\n",
      "üîç Searching for: 'Risk factors for technology companies'\n",
      "\n",
      "üìã Top 2 Results:\n",
      "\n",
      "1. [AAPL] Apple Inc. 10-K Filing 2023\n",
      "   Similarity: 0.453\n",
      "   Content: % compared to fiscal 2022. iPhone revenue was $200.6 billion, Services revenue was $85.2 billion, and Mac revenue was $29.4 billion. RISK FACTORS The ...\n",
      "\n",
      "2. [MSFT] Microsoft Corporation Earnings Report Q4 2023\n",
      "   Similarity: 0.276\n",
      "   Content: ure and other cloud services revenue increased 26%. OUTLOOK The company expects continued growth driven by cloud services adoption and AI integration ...\n",
      "============================================================\n",
      "\n",
      "ü§ñ Financial Q&A System initialized\n",
      "\n",
      "ü§ñ Testing Q&A System\n",
      "==================================================\n",
      "\n",
      "‚ùì Question: What is Apple's revenue for 2023?\n",
      "üí° Answer: Based on the financial documents, revenue figures mentioned include: $200.6 billion, $85.2 billion, $29.4 billion, $56.2 billion for AAPL, MSFT.\n",
      "üìä Confidence: 0.79\n",
      "üìö Sources: AAPL (0.77), AAPL (0.72), MSFT (0.49)\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: How did Microsoft's cloud services perform?\n",
      "üí° Answer: Based on the retrieved financial documents from AAPL, MSFT, here is the relevant information: Microsoft Corporation Quarterly Earnings Report FINANCIAL PERFORMANCE Revenue was $56.2 billion and increased 8% year-over-year. Operating income was $24.3 billion and increased 15% year-over-year. Ne...\n",
      "üìä Confidence: 0.44\n",
      "üìö Sources: MSFT (0.45), MSFT (0.44), AAPL (0.21)\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What are Tesla's vehicle deliveries?\n",
      "üí° Answer: Based on the documents, vehicle deliveries and production metrics show significant growth year-over-year.\n",
      "üìä Confidence: 0.49\n",
      "üìö Sources: TSLA (0.63), TSLA (0.38), AAPL (0.21)\n",
      "--------------------------------------------------\n",
      "\n",
      "‚ùì Question: What risk factors are mentioned for these companies?\n",
      "üí° Answer: Key risk factors mentioned in the documents include global economic conditions, competitive pressures, supply chain disruptions, and regulatory changes.\n",
      "üìä Confidence: 0.39\n",
      "üìö Sources: AAPL (0.45), MSFT (0.27), MSFT (0.25)\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Processed data saved to data\\processed\\embeddings\n",
      "üìä Saved 6 chunks with 384-dimensional embeddings\n",
      "\n",
      "üìã Document Processing Summary\n",
      "==================================================\n",
      "Documents Processed: 3\n",
      "Total Chunks Created: 6\n",
      "Average Chunk Length: 363 characters\n",
      "Embedding Dimension: 384\n",
      "Vector Store Size: 9.0 KB\n",
      "Q&A System: Ready\n",
      "\n",
      "üéØ Next Steps:\n",
      "  ‚úÖ Document processing pipeline is ready\n",
      "  üîÑ Can process PDF files and extract text\n",
      "  üß† Embeddings generated for semantic search\n",
      "  üîç Vector store enables fast similarity search\n",
      "  ü§ñ Basic Q&A system functional\n",
      "  üìä Ready for integration with FinDocGPT API\n",
      "\n",
      "üéâ Document processing pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Document Processing for FinDocGPT\n",
    "# This notebook demonstrates the document processing pipeline including:\n",
    "# - PDF text extraction\n",
    "# - Document chunking and embedding\n",
    "# - Vector store creation\n",
    "# - Q&A system setup\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Document processing - using alternative implementations\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# Sentence transformers for embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìÑ Document Processing Pipeline Started\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ===== 1. Load Sample Financial Documents =====\n",
    "\n",
    "# Sample financial document content\n",
    "sample_documents = [\n",
    "    {\n",
    "        'title': 'Apple Inc. 10-K Filing 2023',\n",
    "        'content': '''\n",
    "        Apple Inc. Annual Report (Form 10-K)\n",
    "        \n",
    "        BUSINESS OVERVIEW\n",
    "        Apple Inc. (\"Apple\" or the \"Company\") designs, manufactures and markets smartphones, personal computers, tablets, wearables and accessories, and sells a variety of related services. The Company's fiscal year is the 52 or 53-week period that ends on the last Saturday of September.\n",
    "        \n",
    "        FINANCIAL HIGHLIGHTS\n",
    "        Net sales for fiscal 2023 were $383.3 billion, a decrease of 3% compared to fiscal 2022. iPhone revenue was $200.6 billion, Services revenue was $85.2 billion, and Mac revenue was $29.4 billion.\n",
    "        \n",
    "        RISK FACTORS\n",
    "        The Company's business is subject to risks including global economic conditions, competitive pressures, supply chain disruptions, and regulatory changes in key markets.\n",
    "        ''',\n",
    "        'metadata': {'company': 'AAPL', 'year': 2023, 'document_type': '10-K'}\n",
    "    },\n",
    "    {\n",
    "        'title': 'Microsoft Corporation Earnings Report Q4 2023',\n",
    "        'content': '''\n",
    "        Microsoft Corporation Quarterly Earnings Report\n",
    "        \n",
    "        FINANCIAL PERFORMANCE\n",
    "        Revenue was $56.2 billion and increased 8% year-over-year. Operating income was $24.3 billion and increased 15% year-over-year. Net income was $20.1 billion and increased 20% year-over-year.\n",
    "        \n",
    "        SEGMENT PERFORMANCE\n",
    "        Productivity and Business Processes revenue increased 12% to $18.3 billion. Intelligent Cloud revenue increased 15% to $24.0 billion. Azure and other cloud services revenue increased 26%.\n",
    "        \n",
    "        OUTLOOK\n",
    "        The company expects continued growth driven by cloud services adoption and AI integration across product portfolio.\n",
    "        ''',\n",
    "        'metadata': {'company': 'MSFT', 'year': 2023, 'document_type': 'Earnings'}\n",
    "    },\n",
    "    {\n",
    "        'title': 'Tesla Inc. Annual Report 2023',\n",
    "        'content': '''\n",
    "        Tesla Inc. Annual Report\n",
    "        \n",
    "        BUSINESS OVERVIEW\n",
    "        Tesla designs, develops, manufactures, sells and leases high-performance fully electric vehicles and energy generation and storage systems.\n",
    "        \n",
    "        PRODUCTION AND DELIVERIES\n",
    "        Total vehicle deliveries were 1.81 million in 2023, representing a 38% increase year-over-year. Model Y was the best-selling vehicle globally in 2023.\n",
    "        \n",
    "        FINANCIAL RESULTS\n",
    "        Total revenue was $96.8 billion, an increase of 19% compared to 2022. Automotive revenue was $82.4 billion. Energy generation and storage revenue was $6.0 billion.\n",
    "        \n",
    "        FUTURE OUTLOOK\n",
    "        The company aims to achieve 20 million vehicle deliveries annually by 2030 through expanded manufacturing capacity and new product introductions.\n",
    "        ''',\n",
    "        'metadata': {'company': 'TSLA', 'year': 2023, 'document_type': 'Annual Report'}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üìö Loaded {len(sample_documents)} sample financial documents\")\n",
    "for doc in sample_documents:\n",
    "    print(f\"  - {doc['title']} ({len(doc['content'])} characters)\")\n",
    "\n",
    "# ===== 2. Text Processing and Chunking =====\n",
    "\n",
    "# Custom text splitter implementation (replacing LangChain dependency)\n",
    "class CustomTextSplitter:\n",
    "    def __init__(self, chunk_size=500, chunk_overlap=50, separators=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = separators or [\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        \"\"\"Split text into chunks with overlap\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Try to split by separators in order of preference\n",
    "        for separator in self.separators:\n",
    "            if separator in text:\n",
    "                parts = text.split(separator)\n",
    "                current_chunk = \"\"\n",
    "                \n",
    "                for part in parts:\n",
    "                    # If adding this part would exceed chunk size, save current chunk\n",
    "                    if len(current_chunk) + len(part) + len(separator) > self.chunk_size and current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                        # Start new chunk with overlap from end of previous chunk\n",
    "                        if self.chunk_overlap > 0:\n",
    "                            overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else current_chunk\n",
    "                            current_chunk = overlap_text + separator + part\n",
    "                        else:\n",
    "                            current_chunk = part\n",
    "                    else:\n",
    "                        # Add part to current chunk\n",
    "                        if current_chunk:\n",
    "                            current_chunk += separator + part\n",
    "                        else:\n",
    "                            current_chunk = part\n",
    "                \n",
    "                # Add final chunk\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                \n",
    "                # If we successfully split the text, return chunks\n",
    "                if len(chunks) > 1:\n",
    "                    return chunks\n",
    "        \n",
    "        # If no separator worked well, split by character count\n",
    "        if len(text) <= self.chunk_size:\n",
    "            return [text]\n",
    "        \n",
    "        # Character-based splitting as fallback\n",
    "        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):\n",
    "            chunk = text[i:i + self.chunk_size]\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Initialize text splitter for document chunking\n",
    "text_splitter = CustomTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Process documents into chunks\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(f\"Processing: {doc['title']}\")\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', doc['content']).strip()\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = text_splitter.split_text(cleaned_text)\n",
    "    \n",
    "    print(f\"  Created {len(chunks)} chunks\")\n",
    "    \n",
    "    # Add chunks with metadata\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"doc_{i}_chunk_{j}\"\n",
    "        all_chunks.append(chunk)\n",
    "        \n",
    "        metadata = doc['metadata'].copy()\n",
    "        metadata.update({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_index': j,\n",
    "            'title': doc['title'],\n",
    "            'chunk_length': len(chunk)\n",
    "        })\n",
    "        chunk_metadata.append(metadata)\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"üìä Average chunk length: {np.mean([len(chunk) for chunk in all_chunks]):.0f} characters\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\nüìÑ Sample Chunks:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i in range(min(3, len(all_chunks))):\n",
    "    print(f\"\\nChunk {i+1} ({chunk_metadata[i]['chunk_id']}):\")\n",
    "    print(f\"Company: {chunk_metadata[i]['company']}\")\n",
    "    print(f\"Length: {len(all_chunks[i])} characters\")\n",
    "    print(f\"Content: {all_chunks[i][:200]}...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# ===== 3. Generate Embeddings =====\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"\\nüß† Initializing embedding model...\")\n",
    "\n",
    "embedding_model = None\n",
    "vectorizer = None\n",
    "\n",
    "try:\n",
    "    # Use financial domain-specific model if available\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"‚úÖ Loaded all-MiniLM-L6-v2 embedding model\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading SentenceTransformer model: {e}\")\n",
    "    # Fallback to TF-IDF\n",
    "    print(\"Using TF-IDF fallback embedding method\")\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=384, stop_words='english')\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nüîÑ Generating embeddings for {len(all_chunks)} chunks...\")\n",
    "\n",
    "if embedding_model:\n",
    "    embeddings = embedding_model.encode(all_chunks, show_progress_bar=True)\n",
    "    print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")\n",
    "else:\n",
    "    # Simple fallback: use TF-IDF-like approach\n",
    "    embeddings = vectorizer.fit_transform(all_chunks).toarray()\n",
    "    print(f\"‚úÖ Generated TF-IDF embeddings with shape: {embeddings.shape}\")\n",
    "\n",
    "# Analyze embedding quality\n",
    "print(\"\\nüìä Embedding Analysis:\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Number of document chunks: {embeddings.shape[0]}\")\n",
    "\n",
    "# Calculate similarity between chunks\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "print(f\"\\nüîó Similarity Analysis:\")\n",
    "print(f\"Average similarity: {np.mean(similarity_matrix):.3f}\")\n",
    "print(f\"Max similarity (excluding self): {np.max(similarity_matrix - np.eye(len(similarity_matrix))):.3f}\")\n",
    "print(f\"Min similarity: {np.min(similarity_matrix):.3f}\")\n",
    "\n",
    "# Find most similar chunk pairs\n",
    "similar_pairs = []\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i+1, len(similarity_matrix)):\n",
    "        if similarity_matrix[i][j] > 0.7:  # High similarity threshold\n",
    "            similar_pairs.append({\n",
    "                'chunk1': chunk_metadata[i]['chunk_id'],\n",
    "                'chunk2': chunk_metadata[j]['chunk_id'],\n",
    "                'similarity': similarity_matrix[i][j],\n",
    "                'company1': chunk_metadata[i]['company'],\n",
    "                'company2': chunk_metadata[j]['company']\n",
    "            })\n",
    "\n",
    "if similar_pairs:\n",
    "    print(f\"\\nüéØ Found {len(similar_pairs)} highly similar chunk pairs:\")\n",
    "    for pair in similar_pairs[:3]:  # Show top 3\n",
    "        print(f\"  {pair['chunk1']} ‚Üî {pair['chunk2']} (similarity: {pair['similarity']:.3f})\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No highly similar chunks found - good diversity\")\n",
    "\n",
    "# ===== 4. Build Vector Store and Search System =====\n",
    "\n",
    "# Create vector store for efficient similarity search\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self, embeddings, texts, metadata):\n",
    "        self.embeddings = embeddings\n",
    "        self.texts = texts\n",
    "        self.metadata = metadata\n",
    "        \n",
    "    def search(self, query_embedding, k=5):\n",
    "        \"\"\"Find k most similar documents to query\"\"\"\n",
    "        similarities = cosine_similarity([query_embedding], self.embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.texts[idx],\n",
    "                'metadata': self.metadata[idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'index': idx\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = SimpleVectorStore(embeddings, all_chunks, chunk_metadata)\n",
    "print(\"\\n‚úÖ Vector store created successfully\")\n",
    "\n",
    "# Test search functionality\n",
    "def search_documents(query, k=3):\n",
    "    \"\"\"Search for relevant documents given a query\"\"\"\n",
    "    print(f\"\\nüîç Searching for: '{query}'\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    if embedding_model:\n",
    "        query_embedding = embedding_model.encode([query])[0]\n",
    "    else:\n",
    "        # Fallback for TF-IDF\n",
    "        query_vector = vectorizer.transform([query]).toarray()[0]\n",
    "        query_embedding = query_vector\n",
    "    \n",
    "    # Search vector store\n",
    "    results = vector_store.search(query_embedding, k=k)\n",
    "    \n",
    "    print(f\"\\nüìã Top {k} Results:\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"\\n{i+1}. [{result['metadata']['company']}] {result['metadata']['title']}\")\n",
    "        print(f\"   Similarity: {result['similarity']:.3f}\")\n",
    "        print(f\"   Content: {result['text'][:150]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test search with sample queries\n",
    "sample_queries = [\n",
    "    \"What is the revenue for Apple?\",\n",
    "    \"Microsoft cloud services performance\",\n",
    "    \"Tesla vehicle deliveries and production\",\n",
    "    \"Risk factors for technology companies\"\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Testing Search Functionality:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in sample_queries:\n",
    "    search_documents(query, k=2)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# ===== 5. Question-Answering System =====\n",
    "\n",
    "# Simple Q&A system using retrieved context\n",
    "class FinancialQASystem:\n",
    "    def __init__(self, vector_store, embedding_model=None, vectorizer=None):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_model = embedding_model\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def answer_question(self, question, max_context_length=1000):\n",
    "        \"\"\"Answer a question using retrieved context\"\"\"\n",
    "        # Search for relevant context\n",
    "        if self.embedding_model:\n",
    "            query_embedding = self.embedding_model.encode([question])[0]\n",
    "        else:\n",
    "            query_embedding = self.vectorizer.transform([question]).toarray()[0]\n",
    "        \n",
    "        results = self.vector_store.search(query_embedding, k=3)\n",
    "        \n",
    "        # Combine context from top results\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['similarity'] > 0.1:  # Relevance threshold\n",
    "                context_parts.append(result['text'])\n",
    "                sources.append({\n",
    "                    'company': result['metadata']['company'],\n",
    "                    'document': result['metadata']['title'],\n",
    "                    'similarity': result['similarity']\n",
    "                })\n",
    "        \n",
    "        combined_context = ' '.join(context_parts)[:max_context_length]\n",
    "        \n",
    "        # Generate answer based on context (simplified)\n",
    "        answer = self._generate_answer(question, combined_context, sources)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'context': combined_context,\n",
    "            'sources': sources,\n",
    "            'confidence': self._calculate_confidence(results)\n",
    "        }\n",
    "    \n",
    "    def _generate_answer(self, question, context, sources):\n",
    "        \"\"\"Generate answer based on retrieved context\"\"\"\n",
    "        # Simple rule-based answer generation\n",
    "        question_lower = question.lower()\n",
    "        context_lower = context.lower()\n",
    "        \n",
    "        if 'revenue' in question_lower:\n",
    "            # Look for revenue mentions in context\n",
    "            revenue_pattern = r'revenue.*?\\$?([0-9,.]+)\\s*(billion|million)'\n",
    "            matches = re.findall(revenue_pattern, context_lower)\n",
    "            \n",
    "            if matches:\n",
    "                companies = [s['company'] for s in sources]\n",
    "                return f\"Based on the financial documents, revenue figures mentioned include: {', '.join([f'${m[0]} {m[1]}' for m in matches])} for {', '.join(set(companies))}.\"\n",
    "        \n",
    "        elif 'risk' in question_lower:\n",
    "            if 'risk' in context_lower:\n",
    "                return \"Key risk factors mentioned in the documents include global economic conditions, competitive pressures, supply chain disruptions, and regulatory changes.\"\n",
    "        \n",
    "        elif any(word in question_lower for word in ['delivery', 'deliveries', 'production']):\n",
    "            if any(word in context_lower for word in ['delivery', 'deliveries', 'million']):\n",
    "                return \"Based on the documents, vehicle deliveries and production metrics show significant growth year-over-year.\"\n",
    "        \n",
    "        # Default response\n",
    "        return f\"Based on the retrieved financial documents from {', '.join(set([s['company'] for s in sources]))}, here is the relevant information: {context[:200]}...\"\n",
    "    \n",
    "    def _calculate_confidence(self, results):\n",
    "        \"\"\"Calculate confidence score based on search results\"\"\"\n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        # Average similarity of top results\n",
    "        avg_similarity = np.mean([r['similarity'] for r in results[:3]])\n",
    "        return min(avg_similarity * 1.2, 1.0)  # Boost slightly but cap at 1.0\n",
    "\n",
    "# Initialize Q&A system\n",
    "qa_system = FinancialQASystem(vector_store, embedding_model, vectorizer)\n",
    "print(\"\\nü§ñ Financial Q&A System initialized\")\n",
    "\n",
    "# Test Q&A system\n",
    "test_questions = [\n",
    "    \"What is Apple's revenue for 2023?\",\n",
    "    \"How did Microsoft's cloud services perform?\",\n",
    "    \"What are Tesla's vehicle deliveries?\",\n",
    "    \"What risk factors are mentioned for these companies?\"\n",
    "]\n",
    "\n",
    "print(\"\\nü§ñ Testing Q&A System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in test_questions:\n",
    "    result = qa_system.answer_question(question)\n",
    "    \n",
    "    print(f\"\\n‚ùì Question: {result['question']}\")\n",
    "    print(f\"üí° Answer: {result['answer']}\")\n",
    "    print(f\"üìä Confidence: {result['confidence']:.2f}\")\n",
    "    sources_text = ', '.join([f\"{s['company']} ({s['similarity']:.2f})\" for s in result['sources']])\n",
    "    print(f\"üìö Sources: {sources_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# ===== 6. Save Processed Data =====\n",
    "\n",
    "# Save embeddings and metadata for later use\n",
    "import pickle\n",
    "\n",
    "# Create processed data structure\n",
    "processed_data = {\n",
    "    'embeddings': embeddings,\n",
    "    'chunks': all_chunks,\n",
    "    'metadata': chunk_metadata,\n",
    "    'embedding_model': 'all-MiniLM-L6-v2' if embedding_model else 'TF-IDF',\n",
    "    'chunk_size': 500,\n",
    "    'chunk_overlap': 50\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('data/processed/embeddings')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save data\n",
    "try:\n",
    "    with open(output_dir / 'document_embeddings.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "\n",
    "    # Save metadata as JSON for easy inspection\n",
    "    with open(output_dir / 'chunk_metadata.json', 'w') as f:\n",
    "        json.dump(chunk_metadata, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ Processed data saved to {output_dir}\")\n",
    "    print(f\"üìä Saved {len(all_chunks)} chunks with {embeddings.shape[1]}-dimensional embeddings\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not save data - {e}\")\n",
    "    print(\"Data is still available in memory for this session\")\n",
    "\n",
    "# ===== 7. Summary and Next Steps =====\n",
    "\n",
    "# Generate processing summary\n",
    "print(\"\\nüìã Document Processing Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary_stats = {\n",
    "    'Documents Processed': len(sample_documents),\n",
    "    'Total Chunks Created': len(all_chunks),\n",
    "    'Average Chunk Length': f\"{np.mean([len(chunk) for chunk in all_chunks]):.0f} characters\",\n",
    "    'Embedding Dimension': embeddings.shape[1],\n",
    "    'Vector Store Size': f\"{embeddings.nbytes / 1024:.1f} KB\",\n",
    "    'Q&A System': 'Ready'\n",
    "}\n",
    "\n",
    "for metric, value in summary_stats.items():\n",
    "    print(f\"{metric}: {value}\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"‚úÖ Document processing pipeline is ready\",\n",
    "    \"üîÑ Can process PDF files and extract text\",\n",
    "    \"üß† Embeddings generated for semantic search\",\n",
    "    \"üîç Vector store enables fast similarity search\",\n",
    "    \"ü§ñ Basic Q&A system functional\",\n",
    "    \"üìä Ready for integration with FinDocGPT API\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "print(\"\\nüéâ Document processing pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "findocgpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
